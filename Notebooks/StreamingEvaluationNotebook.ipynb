{"cells":[{"cell_type":"code","source":["from pyspark import SparkContext\nfrom pyspark.streaming import StreamingContext\nfrom pyspark.streaming.kafka import KafkaUtils\nfrom pyspark.sql import SQLContext\nimport json\nfrom urllib.request import Request, urlopen\nfrom pyspark.sql import SparkSession\nimport pandas as pd\nfrom pyspark.sql.types import StructType, IntegerType, FloatType, StringType\nfrom pyspark.sql.functions import *\n\nimport statsmodels.api as sm\nimport numpy as np\n\nimport os.path\nimport sys\nimport os\nfrom azure.storage.blob import AppendBlobService\nfrom azure.storage import CloudStorageAccount\n\nimport adal\n\n# Working parameters\nstreamingFrequency = 10\nwindowFrequency = 10  # Must be a multiplier of streamingFrequency\ntopic = 'timeseries'\nvariables = [[1, 'Temperature'], [2, 'Load'], [3, 'Voltage']]\nmodelPath = \"linear_regression_model.pickle\"\nmodelsContainer = \"models\"\nfilePrefix = \"timeseries\"\nfileExtension = \"csv\"\ndata_container_name = 'timeseries'\ndatasetName = 'NTEDemoStreaming'\nmodelDate = None\nmodel = None\npredictionModel = None"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Assign Parameters\naccount_name = '{AZURE_STORAGE_ACCOUNT_NAME}'\naccount_key = '{AZURE_STORAGE_ACCOUNT_PASSWORD}'\ntenantId = '{TENANT_ID}'\nclientId = '{CLIENT_ID}'\nusername = '{USERNAME}'\npassword = '{PASSWORD}'\nworkspaceName = '{WORKSPACE_NAME}'\nbrokers = '{BROKER_1_IP_ADDRESS}:9092,{BROKER_2_IP_ADDRESS}:9092,{BROKER_3_IP_ADDRESS}:9092,{BROKER_4_IP_ADDRESS}:9092'"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["append_blob_service = AppendBlobService(account_name=account_name,\n                                        account_key=account_key)\nblob_account = CloudStorageAccount(account_name, account_key)\nblob_service = blob_account.create_block_blob_service()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["def convertToPandas(rdd):\n    pdg = rdd.toPandas().groupby(['ObjectKey'])\n    return pdg\n\ndef loadModel():\n    import sys\n    import pandas\n    sys.modules['pandas.core.indexes'] = pandas.indexes\n    global predictionModel\n    global modelDate\n\n    blob = blob_service.get_blob_properties(modelsContainer, modelPath)\n    storageDate = blob.properties.last_modified\n    if (modelDate is None) or (modelDate < storageDate):\n        print('DOWNLOADING MODEL')\n        # download from storage\n        blob_service.get_blob_to_path(modelsContainer, modelPath, modelPath)\n        predictionModel = sm.load(modelPath)\n        modelDate = storageDate\n    return predictionModel\n\n\ndef predict(data, dsmatrix):\n    import pandas \n    import sys\n    import random\n    sys.modules['pandas.core.indexes'] = pandas.indexes\n    model = loadModel()\n    predictedVariable = variables[0][1]\n    prediction = model.predict(dsmatrix, transform=False)\n    predictionError = data[predictedVariable][0] - prediction[0]\n    if predictionError < 0:\n      predictionError *= -1\n    pred = data.assign(prediction=prediction,\n                       error=predictionError)\n    pred = pred.assign(anomaly=0)\n    isAnomaly = 0\n    \n    for ind in range(len(pred)):\n        current_value = pred.loc[pred.index[ind], predictedVariable]\n        # IN CASE WE HAVE HARD LIMITS FOR VARIABLE VALUES\n        #if (current_value > UPPER_LIMIT) or (current_value < LOWER_LIMIT):\n        #    pred.loc[pred.index[ind],'anomaly'] = True\n        #    continue\n        #elif ind > 1:\n        sigma = random.uniform(0.2, 0.8)\n        prediction = pred.loc[pred.index[ind], 'prediction']\n        prediction_upper = prediction + 5*sigma\n        prediction_lower = prediction - 5*sigma\n\n        if ((current_value > prediction_upper) or (current_value < prediction_lower)):\n           isAnomaly = 1\n    return isAnomaly\n\n\ndef save(data, deviceId, filePrefix, extension):\n    date = data.ValueTime[0][:10]\n    fileName = filePrefix + '-' + date + '.' + extension\n    path = os.path.join(str(deviceId), fileName)\n    dataAsText = data.to_string(header=False, index=False)\n\n    print('Saving ' + path + ' in container ' + data_container_name)\n    append_blob_service.create_container(data_container_name)\n    if not append_blob_service.exists(data_container_name, path):\n        append_blob_service.create_blob(data_container_name, path)\n    append_blob_service.append_blob_from_text(data_container_name,\n                                              path, dataAsText + '\\n')\n\n\ndef getPowerBIToken(username, password, tenantId, clientId):\n    tokenResource = 'https://analysis.windows.net/powerbi/api'\n    tokenEndpoint = 'https://login.windows.net/' + tenantId\n    context = adal.AuthenticationContext(tokenEndpoint)\n    tokenResponse = context.acquire_token_with_username_password(\n        tokenResource,\n        username,\n        password,\n        clientId)\n    token = 'Bearer ' + tokenResponse['accessToken']\n    return token\n\n\ndef callPowerBIApi(access_token, endpoint):\n    addRowsRequest = Request(endpoint)\n    addRowsRequest.add_header('Authorization', access_token)\n    addRowsRequest.add_header('Content-Type', 'application/json')\n    responsePowerBI = urlopen(addRowsRequest).read().decode('utf-8')\n    return responsePowerBI\n\n\ndef getDataSets(access_token, workspaceId):\n    endpoint = ('https://api.powerbi.com/v1.0/myorg/groups/'\n                + workspaceId\n                + '/datasets') \n    return callPowerBIApi(access_token, endpoint)\n\ndef getDataSets(access_token):\n    endpoint = ('https://api.powerbi.com/v1.0/myorg/datasets') \n    return callPowerBIApi(access_token, endpoint)\n  \ndef getDataSetsByWorkspace(access_token, workspaceId):\n    endpoint = ('https://api.powerbi.com/v1.0/myorg/groups/'\n                + workspaceId\n                + '/datasets') \n    return callPowerBIApi(access_token, endpoint)\n\ndef getWorkspaces(access_token):\n    endpoint = 'https://api.powerbi.com/v1.0/myorg/groups'\n    return callPowerBIApi(access_token, endpoint)\n\n\ndef findByName(itemlist, name):\n    return [item for item in itemlist if item['name'] == name]\n\n\ndef sendToPowerBIByWorkspace(data, access_token, workspaceId, datasetId):\n    rows = data.to_json(orient='records')\n    powerBiDatasetEndpoint = ('https://api.powerbi.com/v1.0/myorg/groups/'\n                              + workspaceId + '/datasets/'\n                              + datasetId + '/tables/RealTimeData/rows')\n    addRowsBody = \"{ 'rows' : \" + rows + \" }\"\n    addRowsRequest = Request(powerBiDatasetEndpoint,\n                             addRowsBody.encode(\"utf-8\"))\n    addRowsRequest.add_header('Authorization', access_token)\n    addRowsRequest.add_header('Content-Type', 'application/json')\n    urlopen(addRowsRequest).read()\n\n\ndef sendToPowerBI(data, access_token, datasetId):\n    rows = data.to_json(orient='records')\n    powerBiDatasetEndpoint = ('https://api.powerbi.com/v1.0/myorg/datasets/'\n                              + datasetId + '/tables/RealTimeData/rows')\n    addRowsBody = \"{ 'rows' : \" + rows + \" }\"\n    addRowsRequest = Request(powerBiDatasetEndpoint,\n                             addRowsBody.encode(\"utf-8\"))\n    addRowsRequest.add_header('Authorization', access_token)\n    addRowsRequest.add_header('Content-Type', 'application/json')\n    urlopen(addRowsRequest).read()\n\n\ndef process(ValueTime, Temperature, Load, Voltage, DeviceId):\n    import patsy\n    predictionData = pd.DataFrame({\"ValueTime\":ValueTime,\"Temperature\":Temperature,\"Load\":Load,\"Voltage\":Voltage,\"DeviceId\":DeviceId}, index=[0])\n    x = patsy.dmatrix(\"Load * Voltage\", data=predictionData)\n    \n    pred = predict(predictionData, x)\n    predictedData = predictionData.assign(anomaly=pred)\n    \n    if workspaceId:\n      sendToPowerBIByWorkspace(predictedData, powerBIToken, workspaceId, datasetId)\n    else:\n      sendToPowerBI(predictedData, powerBIToken, datasetId)\n    \n    return pred"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["powerBIToken = getPowerBIToken(username, password, tenantId, clientId)\n\nworkspaceId = ''\ndataset = None\nif workspaceName:\n    workspaces = json.loads(getWorkspaces(powerBIToken))['value']\n    workspace = findByName(workspaces, workspaceName)\n\n    if not workspace:\n        print('Cannot find workspace ' + workspaceName)\n        sys.exit()\n\n    workspaceId = workspace[0]['id']\n    print(\"Using PowerBI workspace \" + workspaceId)\n\n    datasets = json.loads(getDataSetsByWorkspace(powerBIToken, workspaceId))['value']\n    dataset = findByName(datasets, datasetName)\nelse:\n    datasets = json.loads(getDataSets(powerBIToken))['value']\n    dataset = findByName(datasets, datasetName)\n    \nif not dataset:\n    print('Cannot find dataset ' + datasetName)\n    sys.exit()\n\ndatasetId = dataset[0]['id']"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["spark = SparkSession(sc)\nssc = StreamingContext(sc, streamingFrequency)\n\nprocAnomaly = udf(process, IntegerType())\nschema = StructType().add(\"ValueTime\", StringType()).add(\"Temperature\", FloatType()).add(\"Load\", FloatType()).add(\"Voltage\", FloatType()).add(\"DeviceId\", StringType())\n\ndf = spark.readStream \\\n.format(\"kafka\") \\\n.option(\"kafka.bootstrap.servers\", brokers) \\\n.option(\"subscribe\", \"timeseriesspearfish\") \\\n.option(\"startingOffsets\", \"latest\") \\\n.load()\n\nselectDF = df\\\n.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\"))\\\n.select(\"parsed_value.*\")\\\n.select(procAnomaly(\"ValueTime\", \"Temperature\", \"Load\", \"Voltage\", \"DeviceId\").alias(\"IsAnomaly\"), \"*\")\n\nquery = selectDF \\\n.writeStream \\\n.format(\"memory\")\\\n.queryName(\"streamingOutput\")\\\n.start()\n\nquery.awaitTermination()"],"metadata":{},"outputs":[],"execution_count":6}],"metadata":{"name":"StreamingEvaluation","notebookId":3262744637547564},"nbformat":4,"nbformat_minor":0}
